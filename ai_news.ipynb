{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2dcb8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded distilbart summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 100, but your input_length is only 76. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n",
      "C:\\Users\\Sachin Hembram\\AppData\\Local\\Temp\\ipykernel_1816\\1330337592.py:169: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"event_title\": \"Delhi Bomb Blast\",\n",
      "  \"generated_at\": \"2025-11-15T07:59:28.268534Z\",\n",
      "  \"timeline\": [\n",
      "    {\n",
      "      \"date\": \"2025-11-12\",\n",
      "      \"event\": \"Delhi blast case highlights| Association of Indian Universities suspends membership of Al-Falah&nbsp;&nbsp;Deccan Herald\",\n",
      "      \"source\": \"Deccan Herald\",\n",
      "      \"url\": \"https://news.google.com/rss/articles/CBMigwJBVV95cUxNRGhSRm5rT3FlZi1KOHFhamJ1RWk4NTFQdnZfdFo0MnlncFZnQmNQaGk5VVZacERUdzc4a0ZLdGNVeG5IT1BVYU5SZWl3UXZFUGhVaVl4eDNIX1U3cWxOSUFpZXBwME5fQk5uRVY0R19UWlVNYWxhbEIyUmd2U09oVVByTWczYkN4Y2dTbWZ6c3FVOEZ3VTlFZmFLUlBJVkYyLUFveWhncW0tNGxsR1ZpXzFUTEFWMGlSZEd5bThCUzlwWHJ4eVctVjBaSGNjczc2NHpzRE95bjBBSXFrOHNEeG1RMWd6MFpFYWxGT01qcWRmUV9mTV90SVZuOHZ5eXZudmJz?oc=5\"\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"2025-11-12\",\n",
      "      \"event\": \"Do US remarks on Delhi, Islamabad blasts reveal Trump's bias?&nbsp;&nbsp;India Today\",\n",
      "      \"source\": \"India Today\",\n",
      "      \"url\": \"https://news.google.com/rss/articles/CBMi_wFBVV95cUxNM3lWQU5wbTZZeFM3aWRWWGZGRjV2OTVpY2VZd1YwaFNkak5tLTV1UFozMUJpaXU0RDZZdmRabTVGaWxXWU1kZmJ2a1dPV1ViVzVFNHhBbWN1VHZwTE5BdGVnOXU4ZUhEMmlLMjd1UGFzUWhyTzFtUXIwOW8xdEc1ZGhybnVCVVFQbElxMWxCV1RHb1FOOEpKSjZJbmI5NGRmZzR4QTZVblVBenloaTk4YWZFQl84bExkdVc4Y1UxLW5vajZ6elNlZzZrXzJUMXV1MDNfcVdTdEZOVG1wbmtCaXdpaHo1Q1dpVVlBM3dDcjlqYl9lRTRHYTQxakR0bzg?oc=5\"\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"2025-11-12\",\n",
      "      \"event\": \"Delhi Blast: Did police claim CNG cylinder led to explosion?\",\n",
      "      \"source\": \"Alt News\",\n",
      "      \"url\": \"https://news.google.com/rss/articles/CBMixwFBVV95cUxOZXJFTi1XX1VGSFlQUXFmT2tzUnBaUE91cGlxVjVFMWZUSktyNmdFaEtzcnI4MC01MnVEaWlYclY3akM1Z0RBLW02THZBT0tKV2l4bWRjSWlwTjRrbmc0QUFha1VFdUZOQWw0TFhzbjVFUWlsS1hiREh4dm1IMGRETURibGZRRHQtc2VXSC1Bcng1WWl6Tm81V1Q0LVhuNEdUdmxVM0tBSHZXMUswaGFmbWtPZFhKWUc0V2lGTlZMNkVTRDg1cnhj?oc=5\"\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"2025-11-13\",\n",
      "      \"event\": \"\\\"Asli Kaam After 4 pm,\\\" Terror-Accused Doctor Shaheen Told Colleagues&nbsp;&nbsp;NDTV\",\n",
      "      \"source\": \"NDTV\",\n",
      "      \"url\": \"https://news.google.com/rss/articles/\n"
     ]
    }
   ],
   "source": [
    "# AI NEWS ORCHESTRATOR (RSS VERSION) — FULL FIXED\n",
    "# Requirements:\n",
    "# pip install requests pandas beautifulsoup4 transformers sentencepiece spacy dateparser python-dateutil\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import dateparser\n",
    "from dateparser.search import search_dates\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import traceback\n",
    "\n",
    "print(\"Libraries imported\")\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 1 — Fetch News via Google News RSS\n",
    "# -------------------------------\n",
    "def fetch_news_rss(topic, max_articles=10):\n",
    "    topic_query = topic.replace(\" \", \"+\")\n",
    "    rss_url = f\"https://news.google.com/rss/search?q={topic_query}\"\n",
    "\n",
    "    r = requests.get(rss_url, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.content, \"xml\")\n",
    "\n",
    "    items = soup.find_all(\"item\")[:max_articles]\n",
    "\n",
    "    articles = []\n",
    "    for item in items:\n",
    "        title = item.title.text if item.title else \"\"\n",
    "        link = item.link.text if item.link else \"\"\n",
    "        pub_date = item.pubDate.text if item.pubDate else None\n",
    "        description = item.description.text if item.description else \"\"\n",
    "        source = item.source.text if item.source else \"Unknown\"\n",
    "\n",
    "        articles.append({\n",
    "            \"title\": title,\n",
    "            \"url\": link,\n",
    "            \"date\": pub_date,\n",
    "            \"content\": description,\n",
    "            \"source\": source\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(articles)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 2 — Clean Article Text\n",
    "# -------------------------------\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except Exception as e:\n",
    "    print(\"spaCy model not found. Run: python -m spacy download en_core_web_sm\")\n",
    "    raise\n",
    "\n",
    "def clean_text(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_date(date_str):\n",
    "    try:\n",
    "        return dateparser.parse(date_str)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 3 — Summarize Articles (HuggingFace DistilBART)\n",
    "# -------------------------------\n",
    "try:\n",
    "    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "    print(\"Loaded distilbart summarizer\")\n",
    "except Exception as e:\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    print(\"Loaded BART summarizer\")\n",
    "\n",
    "def summarize_article(text, max_len=120, min_len=30):\n",
    "    text = text or \"\"\n",
    "    if len(text.split()) < 20:\n",
    "        return text\n",
    "    try:\n",
    "        words = text.split()\n",
    "        if len(words) > 800:\n",
    "            chunks = [\" \".join(words[i:i+500]) for i in range(0, len(words), 500)]\n",
    "            chunk_summaries = []\n",
    "            for c in chunks:\n",
    "                out = summarizer(c, max_length=max_len, min_length=min_len, do_sample=False)[0]['summary_text']\n",
    "                chunk_summaries.append(out)\n",
    "            joined = \" \".join(chunk_summaries)\n",
    "            final = summarizer(joined, max_length=max_len, min_length=min_len, do_sample=False)[0]['summary_text']\n",
    "            return final\n",
    "        else:\n",
    "            return summarizer(text, max_length=max_len, min_length=min_len, do_sample=False)[0]['summary_text']\n",
    "    except Exception as ex:\n",
    "        print(\"Summarization failed:\", ex)\n",
    "        return text\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 4 — Event Extraction (Robust)\n",
    "# -------------------------------\n",
    "def extract_events(summary_text, published_date=None):\n",
    "    events = []\n",
    "    doc = nlp(summary_text)\n",
    "    for sent in doc.sents:\n",
    "        s = sent.text.strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        # heuristic: check for event keywords\n",
    "        if re.search(r\"\\b(announce|announced|launch|launched|attack|attacked|explode|explosion|arrest|arrived|landed|signed|confirmed|investigate|died|killed)\\b\", s, flags=re.I):\n",
    "            dt = None\n",
    "            found = search_dates(s, languages=['en'])\n",
    "            if found:\n",
    "                dt = found[0][1].date().isoformat()\n",
    "            elif published_date:\n",
    "                dt = published_date.date().isoformat()\n",
    "            events.append({\"event\": s, \"date\": dt})\n",
    "    # fallback: entire summary as single event\n",
    "    if not events:\n",
    "        events.append({\"event\": summary_text.strip(), \"date\": published_date.date().isoformat() if published_date else None})\n",
    "    return events\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 5 — Full Pipeline Function\n",
    "# -------------------------------\n",
    "def process_topic(topic, max_articles=10):\n",
    "    df = fetch_news_rss(topic, max_articles)\n",
    "    df['clean_content'] = df['content'].apply(clean_text)\n",
    "    df['clean_date'] = df['date'].apply(normalize_date)\n",
    "    df['summary'] = df['clean_content'].apply(lambda x: summarize_article(x, max_len=100, min_len=20))\n",
    "    df['events_list'] = df.apply(lambda row: extract_events(row['summary'], row['clean_date']), axis=1)\n",
    "\n",
    "    # Build timeline\n",
    "    timeline = []\n",
    "    for idx, row in df.iterrows():\n",
    "        events = row.get('events_list') or []\n",
    "        for e in events:\n",
    "            if not isinstance(e, dict):\n",
    "                continue\n",
    "            event_text = e.get('event') or \"\"\n",
    "            event_date = e.get('date') if e.get('date') else None\n",
    "            if not event_date and pd.notnull(row['clean_date']):\n",
    "                try:\n",
    "                    event_date = row['clean_date'].date().isoformat()\n",
    "                except:\n",
    "                    event_date = None\n",
    "            timeline.append({\"date\": event_date, \"event\": event_text.strip(), \"source\": row['source'], \"url\": row['url']})\n",
    "\n",
    "    timeline_df = pd.DataFrame(timeline)\n",
    "    if not timeline_df.empty:\n",
    "        timeline_df['date_parsed'] = timeline_df['date'].apply(lambda x: dateparser.parse(x) if x else pd.NaT)\n",
    "        timeline_df = timeline_df.sort_values('date_parsed')\n",
    "\n",
    "    # Source reliability\n",
    "    def score_source(source):\n",
    "        articles = df[df['source']==source]\n",
    "        if len(articles)==0: return 0.0\n",
    "        return float(min(1.0, np.mean(articles['summary'].apply(lambda x: len(str(x).split())))/200.0))\n",
    "    df['reliability'] = df['source'].apply(score_source)\n",
    "\n",
    "    # Final JSON output\n",
    "    final_output = {\n",
    "        \"event_title\": topic,\n",
    "        \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"timeline\": timeline_df[['date','event','source','url']].fillna(\"\").to_dict(orient='records') if not timeline_df.empty else [],\n",
    "        \"summary\": summarize_article(\" \".join(df['summary'].dropna().tolist())) if len(df['summary'].dropna())>0 else \"\",\n",
    "        \"sources\": df[['source','url','reliability']].drop_duplicates().to_dict(orient='records')\n",
    "    }\n",
    "\n",
    "    return df, timeline_df, final_output\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 6 — Run Example\n",
    "# -------------------------------\n",
    "topic = \"Delhi Bomb Blast\"\n",
    "df, timeline_df, final_output = process_topic(topic)\n",
    "\n",
    "print(json.dumps(final_output, indent=2)[:2000])  # preview first 2k chars\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
